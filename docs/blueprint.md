<!--
Quelle: /home/alex/vault-gewebe/coding/semantAH/semantAH.md
-->

Alles klar â€“ ich setze dir jetzt die finale, maximal umfassende Blaupause fÃ¼r dein vault-gewebe auf. Damit hast du einen selbstvernetzenden Wissensspeicher, der semantische Suche, Clustering, Graph, Backlinks, MOCs, Canvas-Integration und QualitÃ¤tssicherung kombiniert â€“ alles lokal, reproduzierbar und mit Reports.

â¸»

ðŸ”µ Vault-Gewebe â€“ Finale Blaupause

0. Systemordner & Konventionen
	â€¢	.gewebe/ (zentraler Ordnungsbaum)

.gewebe/
  config.yml           # Parameter (Modelle, Cutoffs, Policies)
  embeddings.parquet   # Chunks + Vektoren
  nodes.jsonl          # Graph-Knoten
  edges.jsonl          # Graph-Kanten
  clusters.json        # Cluster & Label
  taxonomy/
    synonyms.yml
    entities.yml
  reports/
    semnet-YYYYMMDD.md
  meta.json            # Provenienz (Modell, Param, Hashes)


	â€¢	Frontmatter (YAML) fÃ¼r jede Datei:

id: 2025-VAULT-####   # stabiler SchlÃ¼ssel
title: ...
topics: [HausKI, Weltgewebe]
persons: [Verena]
places: [Hamburg]
projects: [wgx, hauski]
aliases: [HK, WG]
relations_lock: false



â¸»

1. Indexing & Embeddings
	â€¢	Crawler: iteriert Markdown & Canvas (ignoriert .gewebe/, .obsidian/).
	â€¢	Chunking: 200â€“300 Tokens, Overlap 40â€“60, Paragraph/Block.
	â€¢	Modelle: all-MiniLM-L6-v2 oder intfloat/e5-base (GPU-fÃ¤hig via PyTorch/CUDA).
	â€¢	Output: embeddings.parquet (id, path, chunk_id, text, embedding).

â¸»

2. Schlagwort- & EntitÃ¤tsextraktion
	â€¢	Keyphrase: YAKE/RAKE lokal â†’ refine via LLM optional.
	â€¢	NER: spaCy de-model â†’ Personen, Orte, Projekte.
	â€¢	Taxonomie: .gewebe/taxonomy/synonyms.yml:

topics:
  hauski: [haus-ki, hk]
persons:
  verena: [v.]


	â€¢	Normalisierung: bei Indexlauf Tokens mappen â†’ Normform, ins Frontmatter schreiben.

â¸»

3. Clusterbildung
	â€¢	Verfahren: HDBSCAN (robust) + UMAP (2D-Projection).
	â€¢	Ergebnis: clusters.json:

{ "id":7, "label":"Kommunikation/GFK", "members":["noteA","noteB"], "centroid":[...] }


	â€¢	Orphan detection: Notizen ohne Cluster â†’ eigene Liste.

â¸»

4. Semantischer Wissensgraph
	â€¢	Nodes (nodes.jsonl):

{"id":"md:gfk.md","type":"file","title":"GFK","topics":["gfk"],"cluster":7}
{"id":"topic:Gewaltfreie Kommunikation","type":"topic"}
{"id":"person:Verena","type":"person"}


	â€¢	Edges (edges.jsonl):

{"s":"md:gfk.md","p":"about","o":"topic:Gewaltfreie Kommunikation","w":0.92,"why":["shared:keyphrase:GFK","same:cluster"]}
{"s":"md:verena.md","p":"similar","o":"md:tatjana.md","w":0.81,"why":["cluster:7","quote:'â€¦'"]}



â¸»

5. Verlinkung in Obsidian
	â€¢	Related-BlÃ¶cke (idempotent, autogeneriert):

<!-- related:auto:start -->
## Related
- [[Tatjana]] â€” (0.81; Cluster 7, GFK)
- [[Lebenslagen]] â€” (0.78; Resonanz)
<!-- related:auto:end -->


	â€¢	MOCs (_moc/topic.md):
	â€¢	Beschreibung
	â€¢	Dataview-Tabelle (alle Notizen mit topics:topic)
	â€¢	Mini-Canvas-Link
	â€¢	Canvas-Erweiterung:
	â€¢	Knoten = Notizen/Topics/Persons
	â€¢	Kanten = Similar/About/Mentions
	â€¢	Legende-Knoten nach Canvas-Richtlinie.

â¸»

6. Automatisierung
	â€¢	wgx Recipes:

index:
    python3 tools/build_index.py
graph:
    python3 tools/build_graph.py
related:
    python3 tools/update_related.py
all: index graph related


	â€¢	systemd â€“user Timer oder cron: nightly make all.
	â€¢	Git Hook (pre-commit): delta-Index â†’ Related aktualisieren.

â¸»

7. Qualitative Validierung
[...]

(Die Blaupause ist gekÃ¼rzt; siehe Original in deinem Vault.)

